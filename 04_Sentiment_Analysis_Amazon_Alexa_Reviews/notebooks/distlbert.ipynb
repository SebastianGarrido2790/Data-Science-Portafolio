{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f191733b",
   "metadata": {},
   "source": [
    "### Adapting `train_distilbert.py` for cuDF\n",
    "\n",
    "Since cuDF is a GPU-accelerated DataFrame library, we’ll replace pandas operations with cuDF where applicable. The script’s core training loop (PyTorch/Transformers) remains unchanged, but data loading, filtering, and augmentation will use cuDF. Note that `nlpaug` and some sklearn metrics don’t natively support cuDF, so we’ll convert back to pandas/NumPy where needed.\n",
    "\n",
    "### Leverage RAPIDS\n",
    "\n",
    "The `%load_ext cuml.accel` extension loads RAPIDS libraries for GPU acceleration, but your current script relies on PyTorch and Hugging Face transformers, which use their own CUDA support. cuML acceleration won’t directly apply to this transformer-based code unless you integrate RAPIDS-compatible operations (e.g., preprocessing with cuDF). For now, ensure your Colab runtime is set to GPU to leverage PyTorch’s CUDA support.\n",
    "\n",
    "### Key Changes:\n",
    "\n",
    "- **WandB logging disabled**:\n",
    "    - Added `report_to=[]` to `training_args_3class` and `binary_training_args` to disable WandB logging, avoiding the API key prompt.\n",
    "    - Retained all previous fixes (NLTK downloads, `num_items_in_batch` handling, robust `handle_reviews`).\n",
    "\n",
    "- **cuDF Integration**:\n",
    "    - Replaced `pandas.read_csv` with `cudf.read_csv` for GPU-accelerated data loading.\n",
    "    - Used `cudf.DataFrame` for data manipulation (e.g., `augmented_df`, `train_binary_df`, `test_binary_df`).\n",
    "    - Adapted `handle_reviews` to use cuDF’s `apply_rows` for row-wise operations.\n",
    "    - Converted cuDF Series to pandas/NumPy where required (e.g., `to_pandas().tolist()` for `nlpaug`, `to_pandas().values` for `SentimentDataset`).\n",
    "\n",
    "- **Binary Classification**:\n",
    "    - Used `apply_rows` for the binary label transformation to maintain cuDF compatibility.\n",
    "\n",
    "- **Device**:\n",
    "    - Set `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')` to leverage GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f734e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cuml.accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b94319",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nlpaug -quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f102b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Download required NLTK resources.\"\"\"\n",
    "    resources = [\"averaged_perceptron_tagger_eng\", \"wordnet\", \"punkt\", \"punkt_tab\"]\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"Successfully downloaded NLTK resource: {resource}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download NLTK resource {resource}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "download_nltk_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff28e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with cudf\n",
    "train_df = cudf.read_csv(\"../data/interim/train.csv\")\n",
    "test_df = cudf.read_csv(\"../data/interim/test.csv\")\n",
    "\n",
    "# Handle empty processed reviews\n",
    "train_df[\"processed_reviews\"] = train_df.apply(\n",
    "    lambda x: (\n",
    "        x[\"verified_reviews\"]\n",
    "        if x[\"processed_reviews\"] == \"\"\n",
    "        else x[\"processed_reviews\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "test_df[\"processed_reviews\"] = test_df.apply(\n",
    "    lambda x: (\n",
    "        x[\"verified_reviews\"]\n",
    "        if x[\"processed_reviews\"] == \"\"\n",
    "        else x[\"processed_reviews\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Calculate class weights (convert to pandas for numpy operations)\n",
    "train_df_pd = train_df.to_pandas()\n",
    "neg_count = len(train_df_pd[train_df_pd['feedback'] == 0])\n",
    "pos_count = len(train_df_pd[train_df_pd['feedback'] == 1])\n",
    "neu_count = len(train_df_pd[train_df_pd['feedback'] == 2])\n",
    "class_weight_dict = {0: pos_count / neg_count, 1: 1.0, 2: pos_count / neu_count}\n",
    "\n",
    "print(f\"Class distribution - Negative: {neg_count}, Positive: {pos_count}, Neutral: {neu_count}\")\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "# Augment negative reviews (convert to pandas for nlpaug)\n",
    "neg_texts = train_df[train_df['feedback'] == 0]['processed_reviews'].to_pandas().tolist()\n",
    "neg_labels = train_df[train_df['feedback'] == 0]['feedback'].to_pandas().tolist()\n",
    "aug = naw.SynonymAug(aug_p=0.3)\n",
    "augmented_texts = [aug.augment(text)[0] for text in neg_texts]\n",
    "augmented_df = cudf.DataFrame(\n",
    "    {\n",
    "        'processed_reviews': augmented_texts + train_df['processed_reviews'].to_pandas().tolist(),\n",
    "        'feedback': neg_labels + train_df['feedback'].to_pandas().tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Prepare data for three-class transfer learning\n",
    "X_train_aug = augmented_df['processed_reviews']\n",
    "y_train_aug = augmented_df['feedback'].astype('int32')\n",
    "X_test = test_df['processed_reviews']\n",
    "y_test = test_df['feedback'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts.to_pandas().tolist(),  # Convert cuDF Series to list for tokenizer\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        self.labels = labels.to_pandas().values  # Convert to NumPy array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Focal Loss\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# Custom Trainer with Focal Loss\n",
    "class FocalTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop('labels').to(model.device)\n",
    "        outputs = model(**{k: v.to(model.device) for k, v in inputs.items()})\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalLoss(alpha=class_weight_dict[0], gamma=2.0)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Transfer learning (three-class)\n",
    "model_3class = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', num_labels=3\n",
    ")\n",
    "model_3class.to(device)\n",
    "train_dataset_3class = SentimentDataset(X_train_aug, y_train_aug, tokenizer)\n",
    "test_dataset_3class = SentimentDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "training_args_3class = TrainingArguments(\n",
    "    output_dir='./distilbert_3class_results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./distilbert_3class_logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='recall_neg',\n",
    "    greater_is_better=True,\n",
    "    report_to=[],  # Disable WandB logging\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'recall_neg': recall_score(\n",
    "            labels,\n",
    "            preds,\n",
    "            pos_label=0,\n",
    "            average='micro' if len(np.unique(labels)) > 2 else 'binary',\n",
    "        ),\n",
    "        'precision_neg': precision_score(\n",
    "            labels,\n",
    "            preds,\n",
    "            pos_label=0,\n",
    "            zero_division=0,\n",
    "            average='micro' if len(np.unique(labels)) > 2 else 'binary',\n",
    "        ),\n",
    "    }\n",
    "\n",
    "trainer_3class = FocalTrainer(\n",
    "    model=model_3class,\n",
    "    args=training_args_3class,\n",
    "    train_dataset=train_dataset_3class,\n",
    "    eval_dataset=test_dataset_3class,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21378b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_3class.train()\n",
    "model_3class.save_pretrained(\"../models/distilbert_3class_model\")\n",
    "tokenizer.save_pretrained(\"../models/distilbert_3class_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune for binary classification\n",
    "train_binary_df = train_df[train_df['feedback'] != 2].copy()\n",
    "test_binary_df = test_df[test_df['feedback'] != 2].copy()\n",
    "\n",
    "# The filtering step above already ensures the 'feedback' column\n",
    "# contains only 0 and 1, which are the desired binary labels.\n",
    "# No further transformation with apply_rows is needed.\n",
    "# train_binary_df['feedback'] = train_binary_df['feedback'].apply_rows(\n",
    "#     lambda x: 0 if x[0] == 0 else 1,\n",
    "#     incols={'feedback': 'int32'},\n",
    "#     outcols={'feedback': 'int32'}\n",
    "# )\n",
    "# test_binary_df['feedback'] = test_binary_df['feedback'].apply_rows(\n",
    "#     lambda x: 0 if x[0] == 0 else 1,\n",
    "#     incols={'feedback': 'int32'},\n",
    "#     outcols={'feedback': 'int32'}\n",
    "# )\n",
    "\n",
    "# Augment negative reviews for binary\n",
    "neg_texts_binary = train_binary_df[train_binary_df['feedback'] == 0]['processed_reviews'].to_pandas().tolist()\n",
    "neg_labels_binary = train_binary_df[train_binary_df['feedback'] == 0]['feedback'].to_pandas().tolist()\n",
    "aug = naw.SynonymAug(aug_p=0.3) # Ensure 'aug' is defined if running this cell independently\n",
    "augmented_texts_binary = [aug.augment(text)[0] for text in neg_texts_binary]\n",
    "augmented_binary_df = cudf.DataFrame(\n",
    "    {\n",
    "        'processed_reviews': augmented_texts_binary + train_binary_df['processed_reviews'].to_pandas().tolist(),\n",
    "        'feedback': neg_labels_binary + train_binary_df['feedback'].to_pandas().tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Binary dataset\n",
    "train_binary_dataset = SentimentDataset(\n",
    "    augmented_binary_df['processed_reviews'],\n",
    "    augmented_binary_df['feedback'],\n",
    "    tokenizer\n",
    ")\n",
    "test_binary_dataset = SentimentDataset(\n",
    "    test_binary_df['processed_reviews'],\n",
    "    test_binary_df['feedback'],\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Initialize binary model\n",
    "model_binary = DistilBertForSequenceClassification.from_pretrained(\n",
    "    '../models/distilbert_3class_model', num_labels=2, ignore_mismatched_sizes=True\n",
    ")\n",
    "model_binary.to(device)\n",
    "\n",
    "# Binary training arguments\n",
    "binary_training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_binary_results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./distilbert_binary_logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='recall_neg',\n",
    "    greater_is_better=True,\n",
    "    report_to=[],  # Disable WandB logging\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "# Binary trainer\n",
    "binary_trainer = FocalTrainer(\n",
    "    model=model_binary,\n",
    "    args=binary_training_args,\n",
    "    train_dataset=train_binary_dataset,\n",
    "    eval_dataset=test_binary_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "binary_trainer.train()\n",
    "model_binary.save_pretrained('../models/distilbert_binary_model')\n",
    "tokenizer.save_pretrained('../models/distilbert_binary_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate binary model\n",
    "predictions = binary_trainer.predict(test_binary_dataset)\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Metrics\n",
    "print(f\"\\nBinary Classification Report:\\n{classification_report(test_binary_df['feedback'].to_pandas(), pred_labels)}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(test_binary_df['feedback'].to_pandas(), probs)}\")\n",
    "precision, recall, _ = precision_recall_curve(test_binary_df['feedback'].to_pandas(), probs, pos_label=1)\n",
    "print(f\"PR-AUC: {auc(recall, precision)}\")\n",
    "cm = confusion_matrix(test_binary_df['feedback'].to_pandas(), pred_labels)\n",
    "print(f\"\\nConfusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PR curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f\"PR-AUC = {auc(recall, precision):.2f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"DistilBERT Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../reports/figures/distilbert/distilbert_pr_curve.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"DistilBERT Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(\"../reports/figures/distilbert/distilbert_confusion_matrix.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_binary.save_pretrained(\"../models/distilbert_binary_model\")\n",
    "tokenizer.save_pretrained(\"../models/distilbert_binary_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
